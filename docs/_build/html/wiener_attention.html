<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Wiener Attention &#8212; Wiener Transformer 1.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=d1102ebc" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css?v=c058f7c8" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=27fed22d" />
    <script src="_static/documentation_options.js?v=f2a433a1"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="&lt;no title&gt;" href="utils.html" />
    <link rel="prev" title="Wiener Transformer" href="wiener_transformer.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="module-wiener_attention.attention_mechanism">
<span id="wiener-attention"></span><h1>Wiener Attention<a class="headerlink" href="#module-wiener_attention.attention_mechanism" title="Link to this heading">¶</a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="wiener_attention.attention_mechanism.WienerSelfAttention">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">wiener_attention.attention_mechanism.</span></span><span class="sig-name descname"><span class="pre">WienerSelfAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_attention/attention_mechanism.html#WienerSelfAttention"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_attention.attention_mechanism.WienerSelfAttention" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Wiener Self-Attention mechanism for transformer models.</p>
<p>This class implements a custom self-attention mechanism using Wiener filters
for similarity computation between query and key vectors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>config</strong> (<em>object</em>) – Configuration object containing model parameters.</p></li>
<li><p><strong>similarity_function</strong> (<em>callable</em>) – Function to compute similarity between query and key vectors.</p></li>
<li><p><strong>gamma</strong> (<em>float</em><em>, </em><em>optional</em>) – Parameter for the similarity function. Defaults to 0.1.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="wiener_attention.attention_mechanism.WienerSelfAttention.num_attention_heads">
<span class="sig-name descname"><span class="pre">num_attention_heads</span></span><a class="headerlink" href="#wiener_attention.attention_mechanism.WienerSelfAttention.num_attention_heads" title="Link to this definition">¶</a></dt>
<dd><p>Number of attention heads.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="wiener_attention.attention_mechanism.WienerSelfAttention.attention_head_size">
<span class="sig-name descname"><span class="pre">attention_head_size</span></span><a class="headerlink" href="#wiener_attention.attention_mechanism.WienerSelfAttention.attention_head_size" title="Link to this definition">¶</a></dt>
<dd><p>Size of each attention head.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="wiener_attention.attention_mechanism.WienerSelfAttention.all_head_size">
<span class="sig-name descname"><span class="pre">all_head_size</span></span><a class="headerlink" href="#wiener_attention.attention_mechanism.WienerSelfAttention.all_head_size" title="Link to this definition">¶</a></dt>
<dd><p>Total size of all attention heads.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="wiener_attention.attention_mechanism.WienerSelfAttention.query">
<span class="sig-name descname"><span class="pre">query</span></span><a class="headerlink" href="#wiener_attention.attention_mechanism.WienerSelfAttention.query" title="Link to this definition">¶</a></dt>
<dd><p>Linear layer for query transformation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Linear</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="wiener_attention.attention_mechanism.WienerSelfAttention.key">
<span class="sig-name descname"><span class="pre">key</span></span><a class="headerlink" href="#wiener_attention.attention_mechanism.WienerSelfAttention.key" title="Link to this definition">¶</a></dt>
<dd><p>Linear layer for key transformation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Linear</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="wiener_attention.attention_mechanism.WienerSelfAttention.value">
<span class="sig-name descname"><span class="pre">value</span></span><a class="headerlink" href="#wiener_attention.attention_mechanism.WienerSelfAttention.value" title="Link to this definition">¶</a></dt>
<dd><p>Linear layer for value transformation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Linear</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="wiener_attention.attention_mechanism.WienerSelfAttention.dropout">
<span class="sig-name descname"><span class="pre">dropout</span></span><a class="headerlink" href="#wiener_attention.attention_mechanism.WienerSelfAttention.dropout" title="Link to this definition">¶</a></dt>
<dd><p>Dropout layer for regularization.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Dropout</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="wiener_attention.attention_mechanism.WienerSelfAttention.similarity_function">
<span class="sig-name descname"><span class="pre">similarity_function</span></span><a class="headerlink" href="#wiener_attention.attention_mechanism.WienerSelfAttention.similarity_function" title="Link to this definition">¶</a></dt>
<dd><p>Function to compute similarity.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>callable</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="wiener_attention.attention_mechanism.WienerSelfAttention.gamma">
<span class="sig-name descname"><span class="pre">gamma</span></span><a class="headerlink" href="#wiener_attention.attention_mechanism.WienerSelfAttention.gamma" title="Link to this definition">¶</a></dt>
<dd><p>Parameter for the similarity function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_attention.attention_mechanism.WienerSelfAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_states</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder_hidden_states</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder_attention_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">past_key_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_attentions</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_attention/attention_mechanism.html#WienerSelfAttention.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_attention.attention_mechanism.WienerSelfAttention.forward" title="Link to this definition">¶</a></dt>
<dd><p>Forward pass of the Wiener Self-Attention mechanism.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_states</strong> (<em>torch.Tensor</em>) – Input hidden states.</p></li>
<li><p><strong>attention_mask</strong> (<em>torch.Tensor</em><em>, </em><em>optional</em>) – Attention mask. Defaults to None.</p></li>
<li><p><strong>head_mask</strong> (<em>torch.Tensor</em><em>, </em><em>optional</em>) – Mask for attention heads. Defaults to None.</p></li>
<li><p><strong>encoder_hidden_states</strong> (<em>torch.Tensor</em><em>, </em><em>optional</em>) – Hidden states from encoder. Defaults to None.</p></li>
<li><p><strong>encoder_attention_mask</strong> (<em>torch.Tensor</em><em>, </em><em>optional</em>) – Attention mask for encoder. Defaults to None.</p></li>
<li><p><strong>past_key_value</strong> (<em>tuple</em><em>, </em><em>optional</em>) – Cached key and value projection states. Defaults to None.</p></li>
<li><p><strong>output_attentions</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether to output attention weights. Defaults to True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><dl class="simple">
<dt>A tuple containing:</dt><dd><ul class="simple">
<li><p>context_layer (torch.Tensor): Output context layer.</p></li>
<li><p>attention_probs (torch.Tensor): Attention probabilities if output_attentions is True.</p></li>
</ul>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_attention.attention_mechanism.WienerSelfAttention.transpose_for_scores">
<span class="sig-name descname"><span class="pre">transpose_for_scores</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_attention/attention_mechanism.html#WienerSelfAttention.transpose_for_scores"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_attention.attention_mechanism.WienerSelfAttention.transpose_for_scores" title="Link to this definition">¶</a></dt>
<dd><p>Transpose and reshape the input tensor for attention score calculation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<em>torch.Tensor</em>) – Input tensor of shape (batch_size, seq_length, all_head_size).</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Reshaped tensor of shape (batch_size, num_attention_heads, seq_length, attention_head_size).</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class" id="module-wiener_attention.wiener_metric">
<dt class="sig sig-object py" id="wiener_attention.wiener_metric.WienerSimilarityMetric">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">wiener_attention.wiener_metric.</span></span><span class="sig-name descname"><span class="pre">WienerSimilarityMetric</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'fft'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'reverse'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">penalty_function</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">store_filters</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">std</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clamp_min</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">corr_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rel_epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_attention/wiener_metric.html#WienerSimilarityMetric"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_attention.wiener_metric.WienerSimilarityMetric" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>This loss was implemented by Cruz et al  in the paper <cite>Convolve and Conquer:
Data Comparison with Wiener Filters</cite>. I have modified the implementation in two
ways:</p>
<ol class="arabic simple">
<li><p>Ensured that the identity function can work with any number of dimensions</p></li>
</ol>
<p>2. Made the function not return an average, but rather a value for each sample
pair in the batch.</p>
<p>Original implementation: <a class="reference external" href="https://github.com/dpelacani/WienerLoss">https://github.com/dpelacani/WienerLoss</a></p>
<p>The AWLoss class implements the adaptive Wiener criterion, which
aims to compare two data samples through a convolutional filter. The
methodology is inspired by the paper <a href="#id1"><span class="problematic" id="id2">`Adaptive Waveform Inversion:
Theory`_</span></a> (Warner and Guasch, 2014).</p>
<p>A matching filter <cite>w</cite> can be computed such that it transforms
a targetsignal <cite>p</cite> into the data <cite>d</cite> under an L2 norm principle:</p>
<p>g = || p*w - d ||^2</p>
<p>Let ‘Z’ be the Toeplitz matrix formulation of <cite>p</cite> such that the
equation above is equivalent to:
g = || Zw - d ||^2</p>
<p>Minimizing this functional:
dgdw = Z^T (Zw - d)
dgdw –&gt; 0 : w = (Z^T &#64; Z)^(-1) &#64; Z^T &#64; d</p>
<p>To stabilize the matrix inversion, an amount is added to the diagonal
of (Z^T &#64; Z) based on a value epsilon such that the inverted matrix is
(Z^T &#64; Z) + max(diagonal(Z^T &#64; Z)) * epsilon</p>
<p>In 2D, convolving p with w (or w with p) is equivalent to the matrix
vector multiplication Zd where Z is the doubly block Toeplitz of the
reconstructed image P and w is the flattened array of the 2D kernel W.</p>
<p>Therefore, the system is equivalent to solving || Zw - d ||^2, and
the solution to w is given by
w = (Z^T &#64; Z + max(diagonal(Z^T &#64; Z)) * epsilon)^(-1) &#64; Z^T &#64; d</p>
<p>This composes the direct method.</p>
<p>Alternatively, convolution can be performed in the frequency domain
with multiplication and division operations.
This tends to be much more computationally efficient.</p>
<p>The criterion is evaluated through a symmetrical monotonically decreasing
function T and a dirac delta function that rewards when the filter kernel
<cite>w</cite> is close to the identity kernel, and penalizes otherwise.</p>
<p>f = 1/2 ||T * (v - delta)||^2</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>method</strong> – “fft” for Fast Fourier Transform or “direct” for the
Levinson-Durbin recurssion algorithm. Defaults to “fft”</p></li>
<li><p><strong>optional</strong> – “fft” for Fast Fourier Transform or “direct” for the
Levinson-Durbin recurssion algorithm. Defaults to “fft”</p></li>
<li><p><strong>filter_dim</strong> – the dimensionality of the filter. This parameter should be
upper-bounded by the dimensionality of the data. If data is
3-dimensional and filter_dim is set to 2, one filter is computed
per channel dimension assuming format [B, NC, H , W]. Current
implementation only supports filter dimensions for 1D, 2D and 3D.
Defaults to 2</p></li>
<li><p><strong>optional</strong> – the dimensionality of the filter. This parameter should be
upper-bounded by the dimensionality of the data. If data is
3-dimensional and filter_dim is set to 2, one filter is computed
per channel dimension assuming format [B, NC, H , W]. Current
implementation only supports filter dimensions for 1D, 2D and 3D.
Defaults to 2</p></li>
<li><p><strong>filter_scale</strong> – the scale of the filters compared to the size of the data.
Defaults to 2</p></li>
<li><p><strong>optional</strong> – the scale of the filters compared to the size of the data.
Defaults to 2</p></li>
<li><p><strong>reduction</strong> – specifies the reduction to apply to the output, “mean” or “sum”.
Defaults to mean</p></li>
<li><p><strong>optional</strong> – specifies the reduction to apply to the output, “mean” or “sum”.
Defaults to mean</p></li>
<li><p><strong>mode</strong> – “forward” or “reverse” computation of the filter. For details of
the difference, refer to the original paper. Default “reverse”</p></li>
<li><p><strong>optional</strong> – “forward” or “reverse” computation of the filter. For details of
the difference, refer to the original paper. Default “reverse”</p></li>
<li><p><strong>penalty_function</strong> – the penalty function to apply to the filter. If None, the penalty
function is the identity. Takes “identity”, “gaussian” or custom
penalty function. Default None</p></li>
<li><p><strong>optional</strong> – the penalty function to apply to the filter. If None, the penalty
function is the identity. Takes “identity”, “gaussian” or custom
penalty function. Default None</p></li>
<li><p><strong>std</strong> – the standard deviation of the gaussian when penalty_function=”gaussian”.
Mean is always zero. Default None</p></li>
<li><p><strong>optional</strong> – the standard deviation of the gaussian when penalty_function=”gaussian”.
Mean is always zero. Default None</p></li>
<li><p><strong>store_filters</strong> – whether to store the filters in memory, useful for debugging.
Option to store the filers before or after normalisation with
“norm” and “unorm”. Default False.</p></li>
<li><p><strong>optional</strong> – whether to store the filters in memory, useful for debugging.
Option to store the filers before or after normalisation with
“norm” and “unorm”. Default False.</p></li>
<li><p><strong>epsilon</strong> – the stabilization value to compute the filter. Default 1e-4.</p></li>
<li><p><strong>optional</strong> – the stabilization value to compute the filter. Default 1e-4.</p></li>
<li><p><strong>clamp_min</strong> – filters are clipped to this minimum value after computation. If
None, operation is disabled. Default none</p></li>
<li><p><strong>optional</strong> – filters are clipped to this minimum value after computation. If
None, operation is disabled. Default none</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="wiener_attention.wiener_metric.WienerSimilarityMetric.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recon</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_attention/wiener_metric.html#WienerSimilarityMetric.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_attention.wiener_metric.WienerSimilarityMetric.forward" title="Link to this definition">¶</a></dt>
<dd><p>&gt; The function takes in a reconstructed signal, a target signal,
and a few other parameters, and returns the loss</p>
<dl class="simple">
<dt>Args</dt><dd><dl class="simple">
<dt>recon</dt><dd><p>the reconstructed signal</p>
</dd>
<dt>target</dt><dd><p>the target signal</p>
</dd>
<dt>epsilon, optional</dt><dd><p>the stabilization value to compute the filter. If passed,
overwrites the class attribute of same name. Default None.</p>
</dd>
<dt>gamma, optional</dt><dd><p>noise to add to both target and reconstructed signals
for training stabilization. Default 0.</p>
</dd>
<dt>eta, optional</dt><dd><p>noise to add to penalty function. Default 0.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_attention.wiener_metric.WienerSimilarityMetric.get_filter_shape">
<span class="sig-name descname"><span class="pre">get_filter_shape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_attention/wiener_metric.html#WienerSimilarityMetric.get_filter_shape"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_attention.wiener_metric.WienerSimilarityMetric.get_filter_shape" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_attention.wiener_metric.WienerSimilarityMetric.identity">
<span class="sig-name descname"><span class="pre">identity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mesh</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_attention/wiener_metric.html#WienerSimilarityMetric.identity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_attention.wiener_metric.WienerSimilarityMetric.identity" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_attention.wiener_metric.WienerSimilarityMetric.make_delta">
<span class="sig-name descname"><span class="pre">make_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_attention/wiener_metric.html#WienerSimilarityMetric.make_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_attention.wiener_metric.WienerSimilarityMetric.make_delta" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_attention.wiener_metric.WienerSimilarityMetric.make_doubly_block">
<span class="sig-name descname"><span class="pre">make_doubly_block</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_attention/wiener_metric.html#WienerSimilarityMetric.make_doubly_block"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_attention.wiener_metric.WienerSimilarityMetric.make_doubly_block" title="Link to this definition">¶</a></dt>
<dd><p>Makes Doubly Blocked Toeplitz of a matrix X [r, c]</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_attention.wiener_metric.WienerSimilarityMetric.make_penalty">
<span class="sig-name descname"><span class="pre">make_penalty</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">penalty_function</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">std</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_attention/wiener_metric.html#WienerSimilarityMetric.make_penalty"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_attention.wiener_metric.WienerSimilarityMetric.make_penalty" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_attention.wiener_metric.WienerSimilarityMetric.make_toeplitz">
<span class="sig-name descname"><span class="pre">make_toeplitz</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">a</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_attention/wiener_metric.html#WienerSimilarityMetric.make_toeplitz"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_attention.wiener_metric.WienerSimilarityMetric.make_toeplitz" title="Link to this definition">¶</a></dt>
<dd><p>Makes toeplitz matrix of a vector A</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_attention.wiener_metric.WienerSimilarityMetric.multigauss">
<span class="sig-name descname"><span class="pre">multigauss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mesh</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">covmatrix</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_attention/wiener_metric.html#WienerSimilarityMetric.multigauss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_attention.wiener_metric.WienerSimilarityMetric.multigauss" title="Link to this definition">¶</a></dt>
<dd><p>Multivariate gaussian of N dimensions on evenly spaced
hypercubed grid. Mesh should be stacked along the last axis
E.g. for a 3D gaussian of 20 grid points in each axis mesh
should be of shape (20, 20, 20, 3)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_attention.wiener_metric.WienerSimilarityMetric.pad_signal">
<span class="sig-name descname"><span class="pre">pad_signal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_attention/wiener_metric.html#WienerSimilarityMetric.pad_signal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_attention.wiener_metric.WienerSimilarityMetric.pad_signal" title="Link to this definition">¶</a></dt>
<dd><p>x must be a multichannel signal of shape
[batch_size, nchannels, width, height]</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_attention.wiener_metric.WienerSimilarityMetric.rms">
<span class="sig-name descname"><span class="pre">rms</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_attention/wiener_metric.html#WienerSimilarityMetric.rms"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_attention.wiener_metric.WienerSimilarityMetric.rms" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_attention.wiener_metric.WienerSimilarityMetric.wiener">
<span class="sig-name descname"><span class="pre">wiener</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-09</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_attention/wiener_metric.html#WienerSimilarityMetric.wiener"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_attention.wiener_metric.WienerSimilarityMetric.wiener" title="Link to this definition">¶</a></dt>
<dd><p>calculates the optimal least squares convolutional Wiener filter that
transforms signal x into signal y using the direct Toeplitz matrix
implementation</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_attention.wiener_metric.WienerSimilarityMetric.wienerfft">
<span class="sig-name descname"><span class="pre">wienerfft</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prwh</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-09</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_attention/wiener_metric.html#WienerSimilarityMetric.wienerfft"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_attention.wiener_metric.WienerSimilarityMetric.wienerfft" title="Link to this definition">¶</a></dt>
<dd><p>George Strong (<a class="reference external" href="mailto:geowstrong&#37;&#52;&#48;gmail&#46;com">geowstrong<span>&#64;</span>gmail<span>&#46;</span>com</a>)
calculates the optimal least squares convolutional Wiener filter that
transforms signal x into signal y using FFT</p>
</dd></dl>

</dd></dl>

<dl class="py function" id="module-wiener_attention.model">
<dt class="sig sig-object py" id="wiener_attention.model.make_bert_model">
<span class="sig-prename descclassname"><span class="pre">wiener_attention.model.</span></span><span class="sig-name descname"><span class="pre">make_bert_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">wiener_attention</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wiener_similarity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_attention/model.html#make_bert_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_attention.model.make_bert_model" title="Link to this definition">¶</a></dt>
<dd><p>Initializes a single-layer BERT model for sequence classification with a custom attention module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>num_labels</strong> (<em>int</em>) – Number of labels for the classification task.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The tokenizer associated with the model.
model: A BERT model instance for sequence classification with custom attention.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tokenizer</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="wiener_attention.model.make_bert_tokenizer">
<span class="sig-prename descclassname"><span class="pre">wiener_attention.model.</span></span><span class="sig-name descname"><span class="pre">make_bert_tokenizer</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_attention/model.html#make_bert_tokenizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_attention.model.make_bert_tokenizer" title="Link to this definition">¶</a></dt>
<dd><p>Initializes a BERT tokenizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The tokenizer associated with the model.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>tokenizer</p>
</dd>
</dl>
</dd></dl>

</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Wiener Transformer</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="wiener_transformer.html">Wiener Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="wiener_transformer.html#utilities">Utilities</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Wiener Attention</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#wiener_attention.attention_mechanism.WienerSelfAttention"><code class="docutils literal notranslate"><span class="pre">WienerSelfAttention</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#wiener_attention.wiener_metric.WienerSimilarityMetric"><code class="docutils literal notranslate"><span class="pre">WienerSimilarityMetric</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#wiener_attention.model.make_bert_model"><code class="docutils literal notranslate"><span class="pre">make_bert_model()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#wiener_attention.model.make_bert_tokenizer"><code class="docutils literal notranslate"><span class="pre">make_bert_tokenizer()</span></code></a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="wiener_transformer.html" title="previous chapter">Wiener Transformer</a></li>
      <li>Next: <a href="utils.html" title="next chapter">&lt;no title&gt;</a></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2024, Andrei Danila.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.0.2</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="_sources/wiener_attention.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>