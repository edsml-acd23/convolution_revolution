<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Wiener Transformer &#8212; Wiener Transformer 1.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=d1102ebc" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css?v=c058f7c8" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css?v=27fed22d" />
    <script src="_static/documentation_options.js?v=f2a433a1"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Wiener Attention" href="wiener_attention.html" />
    <link rel="prev" title="Welcome to Wiener Transformer Documentation" href="index.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="wiener-transformer">
<h1>Wiener Transformer<a class="headerlink" href="#wiener-transformer" title="Link to this heading">¶</a></h1>
<p>This is the implementation of the Transformer model used with Wiener Loss.</p>
<dl class="py class" id="module-wiener_transformer.transformer">
<dt class="sig sig-object py" id="wiener_transformer.transformer.Decoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">wiener_transformer.transformer.</span></span><span class="sig-name descname"><span class="pre">Decoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">N</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#Decoder"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.Decoder" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Generic N layer decoder with masking to prevent attending to future positions.</p>
<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.transformer.Decoder.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_mask</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#Decoder.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.Decoder.forward" title="Link to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="wiener_transformer.transformer.DecoderLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">wiener_transformer.transformer.</span></span><span class="sig-name descname"><span class="pre">DecoderLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">self_attn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_attn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feed_forward</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#DecoderLayer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.DecoderLayer" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Decoder layer is made up of self-attention, source-attention, and feed-forward networks.</p>
<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.transformer.DecoderLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_mask</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#DecoderLayer.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.DecoderLayer.forward" title="Link to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="wiener_transformer.transformer.Embeddings">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">wiener_transformer.transformer.</span></span><span class="sig-name descname"><span class="pre">Embeddings</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vocab_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'learned'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">word2vec_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fasttext_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">glove_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#Embeddings"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.Embeddings" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Embedding layer that can be initialized with pre-trained embeddings (Word2Vec, FastText, GloVe) or learned embeddings.</p>
<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.transformer.Embeddings.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#Embeddings.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.Embeddings.forward" title="Link to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.transformer.Embeddings.init_fasttext_weights">
<span class="sig-name descname"><span class="pre">init_fasttext_weights</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fasttext_model</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#Embeddings.init_fasttext_weights"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.Embeddings.init_fasttext_weights" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.transformer.Embeddings.init_glove_weights">
<span class="sig-name descname"><span class="pre">init_glove_weights</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">glove_weights</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#Embeddings.init_glove_weights"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.Embeddings.init_glove_weights" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.transformer.Embeddings.init_learned_weights">
<span class="sig-name descname"><span class="pre">init_learned_weights</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#Embeddings.init_learned_weights"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.Embeddings.init_learned_weights" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.transformer.Embeddings.init_word2vec_weights">
<span class="sig-name descname"><span class="pre">init_word2vec_weights</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">word2vec_model</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#Embeddings.init_word2vec_weights"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.Embeddings.init_word2vec_weights" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="wiener_transformer.transformer.Encoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">wiener_transformer.transformer.</span></span><span class="sig-name descname"><span class="pre">Encoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">N</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#Encoder"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.Encoder" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Core Encoder is a stack of N identical layers.</p>
<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.transformer.Encoder.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#Encoder.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.Encoder.forward" title="Link to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="wiener_transformer.transformer.EncoderDecoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">wiener_transformer.transformer.</span></span><span class="sig-name descname"><span class="pre">EncoderDecoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">encoder</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_embed</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_embed</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generator</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#EncoderDecoder"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.EncoderDecoder" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>A standard Encoder-Decoder architecture.
The Encoder processes the input sequence, and the Decoder generates the output sequence.</p>
<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.transformer.EncoderDecoder.decode">
<span class="sig-name descname"><span class="pre">decode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memory</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_mask</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#EncoderDecoder.decode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.EncoderDecoder.decode" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.transformer.EncoderDecoder.encode">
<span class="sig-name descname"><span class="pre">encode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_mask</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#EncoderDecoder.encode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.EncoderDecoder.encode" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.transformer.EncoderDecoder.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_mask</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#EncoderDecoder.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.EncoderDecoder.forward" title="Link to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="wiener_transformer.transformer.EncoderLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">wiener_transformer.transformer.</span></span><span class="sig-name descname"><span class="pre">EncoderLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">self_attn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feed_forward</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#EncoderLayer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.EncoderLayer" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Encoder layer is made up of self-attention and feed-forward networks.</p>
<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.transformer.EncoderLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#EncoderLayer.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.EncoderLayer.forward" title="Link to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="wiener_transformer.transformer.Generator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">wiener_transformer.transformer.</span></span><span class="sig-name descname"><span class="pre">Generator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vocab</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#Generator"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.Generator" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Linear layer followed by softmax to generate output probabilities over the target vocabulary.</p>
<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.transformer.Generator.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#Generator.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.Generator.forward" title="Link to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="wiener_transformer.transformer.LayerNorm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">wiener_transformer.transformer.</span></span><span class="sig-name descname"><span class="pre">LayerNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#LayerNorm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.LayerNorm" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Layer Normalization as introduced by Ba et al.</p>
<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.transformer.LayerNorm.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#LayerNorm.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.LayerNorm.forward" title="Link to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="wiener_transformer.transformer.MultiHeadedAttention">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">wiener_transformer.transformer.</span></span><span class="sig-name descname"><span class="pre">MultiHeadedAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">h</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#MultiHeadedAttention"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.MultiHeadedAttention" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Multi-Head Attention mechanism, which allows the model to jointly attend to information from different representation subspaces.</p>
<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.transformer.MultiHeadedAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#MultiHeadedAttention.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.MultiHeadedAttention.forward" title="Link to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="wiener_transformer.transformer.PositionalEncoding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">wiener_transformer.transformer.</span></span><span class="sig-name descname"><span class="pre">PositionalEncoding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5000</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#PositionalEncoding"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.PositionalEncoding" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Add positional encoding to the input embeddings to provide information about the position of the tokens in the sequence.</p>
<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.transformer.PositionalEncoding.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#PositionalEncoding.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.PositionalEncoding.forward" title="Link to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="wiener_transformer.transformer.PositionwiseFeedForward">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">wiener_transformer.transformer.</span></span><span class="sig-name descname"><span class="pre">PositionwiseFeedForward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_ff</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#PositionwiseFeedForward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.PositionwiseFeedForward" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Implements the position-wise feed-forward network.</p>
<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.transformer.PositionwiseFeedForward.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#PositionwiseFeedForward.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.PositionwiseFeedForward.forward" title="Link to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="wiener_transformer.transformer.SublayerConnection">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">wiener_transformer.transformer.</span></span><span class="sig-name descname"><span class="pre">SublayerConnection</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#SublayerConnection"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.SublayerConnection" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>A residual connection followed by a layer normalization.</p>
<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.transformer.SublayerConnection.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sublayer</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#SublayerConnection.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.SublayerConnection.forward" title="Link to this definition">¶</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="wiener_transformer.transformer.clones">
<span class="sig-prename descclassname"><span class="pre">wiener_transformer.transformer.</span></span><span class="sig-name descname"><span class="pre">clones</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">N</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#clones"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.clones" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="wiener_transformer.transformer.make_model">
<span class="sig-prename descclassname"><span class="pre">wiener_transformer.transformer.</span></span><span class="sig-name descname"><span class="pre">make_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src_vocab</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_vocab</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">N</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_ff</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2048</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'learned'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#make_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.make_model" title="Link to this definition">¶</a></dt>
<dd><p>Helper function to construct a model from hyperparameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src_vocab</strong> – Size of source vocabulary.</p></li>
<li><p><strong>tgt_vocab</strong> – Size of target vocabulary.</p></li>
<li><p><strong>N</strong> – Number of layers in the encoder and decoder.</p></li>
<li><p><strong>d_model</strong> – Dimensionality of the embeddings.</p></li>
<li><p><strong>d_ff</strong> – Dimensionality of the feed-forward network.</p></li>
<li><p><strong>h</strong> – Number of attention heads.</p></li>
<li><p><strong>dropout</strong> – Dropout rate.</p></li>
<li><p><strong>embedding_type</strong> – Type of embeddings (‘learned’, ‘word2vec’, ‘fasttext’, ‘glove’).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A constructed EncoderDecoder model.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="wiener_transformer.transformer.subsequent_mask">
<span class="sig-prename descclassname"><span class="pre">wiener_transformer.transformer.</span></span><span class="sig-name descname"><span class="pre">subsequent_mask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/transformer.html#subsequent_mask"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.transformer.subsequent_mask" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="utilities">
<h1>Utilities<a class="headerlink" href="#utilities" title="Link to this heading">¶</a></h1>
<p>These are the utilities associated with the Wiener Loss Transformer.</p>
<section id="module-wiener_transformer.utils.train">
<span id="train"></span><h2>Train<a class="headerlink" href="#module-wiener_transformer.utils.train" title="Link to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="wiener_transformer.utils.train.load_trained_model">
<span class="sig-prename descclassname"><span class="pre">wiener_transformer.utils.train.</span></span><span class="sig-name descname"><span class="pre">load_trained_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/train.html#load_trained_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.train.load_trained_model" title="Link to this definition">¶</a></dt>
<dd><p>Load a trained model from disk. If no trained model exists, train it first.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>config</strong> – Configuration dictionary.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Trained model.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="wiener_transformer.utils.train.train_model">
<span class="sig-prename descclassname"><span class="pre">wiener_transformer.utils.train.</span></span><span class="sig-name descname"><span class="pre">train_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">valid</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_tokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_tokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">accelerator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_scheduler</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/train.html#train_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.train.train_model" title="Link to this definition">¶</a></dt>
<dd><p>Train the model using the specified configuration and dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train</strong> – Training dataset.</p></li>
<li><p><strong>valid</strong> – Validation dataset.</p></li>
<li><p><strong>test</strong> – Test dataset.</p></li>
<li><p><strong>src_tokenizer</strong> – Source tokenizer.</p></li>
<li><p><strong>tgt_tokenizer</strong> – Target tokenizer.</p></li>
<li><p><strong>config</strong> – Configuration dictionary.</p></li>
<li><p><strong>accelerator</strong> – Accelerator object for distributed training (optional).</p></li>
<li><p><strong>model</strong> – Preloaded model (optional).</p></li>
<li><p><strong>optimizer</strong> – Preloaded optimizer (optional).</p></li>
<li><p><strong>lr_scheduler</strong> – Preloaded learning rate scheduler (optional).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-wiener_transformer.utils.embeddings">
<span id="embeddings"></span><h2>Embeddings<a class="headerlink" href="#module-wiener_transformer.utils.embeddings" title="Link to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="wiener_transformer.utils.embeddings.create_embedding_weights">
<span class="sig-prename descclassname"><span class="pre">wiener_transformer.utils.embeddings.</span></span><span class="sig-name descname"><span class="pre">create_embedding_weights</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'word2vec'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vector_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/embeddings.html#create_embedding_weights"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.embeddings.create_embedding_weights" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="wiener_transformer.utils.embeddings.extract_sentences">
<span class="sig-prename descclassname"><span class="pre">wiener_transformer.utils.embeddings.</span></span><span class="sig-name descname"><span class="pre">extract_sentences</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataloader</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/embeddings.html#extract_sentences"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.embeddings.extract_sentences" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="wiener_transformer.utils.embeddings.load_glove_embeddings">
<span class="sig-prename descclassname"><span class="pre">wiener_transformer.utils.embeddings.</span></span><span class="sig-name descname"><span class="pre">load_glove_embeddings</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vector_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/embeddings.html#load_glove_embeddings"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.embeddings.load_glove_embeddings" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="module-wiener_transformer.utils.wienerloss">
<span id="wiener-loss"></span><h2>Wiener Loss<a class="headerlink" href="#module-wiener_transformer.utils.wienerloss" title="Link to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="wiener_transformer.utils.wienerloss.WienerLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">wiener_transformer.utils.wienerloss.</span></span><span class="sig-name descname"><span class="pre">WienerLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'fft'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'reverse'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">penalty_function</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">store_filters</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">std</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clamp_min</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">corr_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rel_epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/wienerloss.html#WienerLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.wienerloss.WienerLoss" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>This loss was implemented by Cruz et al  in the paper <cite>Convolve and Conquer:
Data Comparison with Wiener Filters</cite>. I have modified the implementation:</p>
<ol class="arabic simple">
<li><p>Ensured that the identity function can work with any number of dimensions</p></li>
</ol>
<p>Original implementation: <a class="reference external" href="https://github.com/dpelacani/WienerLoss">https://github.com/dpelacani/WienerLoss</a></p>
<p>The AWLoss class implements the adaptive Wiener criterion, which
aims to compare two data samples through a convolutional filter. The
methodology is inspired by the paper <a href="#id7"><span class="problematic" id="id8">`Adaptive Waveform Inversion:
Theory`_</span></a> (Warner and Guasch, 2014).</p>
<p>A matching filter <cite>w</cite> can be computed such that it transforms
a targetsignal <cite>p</cite> into the data <cite>d</cite> under an L2 norm principle:</p>
<p>g = || p*w - d ||^2</p>
<p>Let ‘Z’ be the Toeplitz matrix formulation of <cite>p</cite> such that the
equation above is equivalent to:
g = || Zw - d ||^2</p>
<p>Minimizing this functional:
dgdw = Z^T (Zw - d)
dgdw –&gt; 0 : w = (Z^T &#64; Z)^(-1) &#64; Z^T &#64; d</p>
<p>To stabilize the matrix inversion, an amount is added to the diagonal
of (Z^T &#64; Z) based on a value epsilon such that the inverted matrix is
(Z^T &#64; Z) + max(diagonal(Z^T &#64; Z)) * epsilon</p>
<p>In 2D, convolving p with w (or w with p) is equivalent to the matrix
vector multiplication Zd where Z is the doubly block Toeplitz of the
reconstructed image P and w is the flattened array of the 2D kernel W.</p>
<p>Therefore, the system is equivalent to solving || Zw - d ||^2, and
the solution to w is given by
w = (Z^T &#64; Z + max(diagonal(Z^T &#64; Z)) * epsilon)^(-1) &#64; Z^T &#64; d</p>
<p>This composes the direct method.</p>
<p>Alternatively, convolution can be performed in the frequency domain
with multiplication and division operations.
This tends to be much more computationally efficient.</p>
<p>The criterion is evaluated through a symmetrical monotonically decreasing
function T and a dirac delta function that rewards when the filter kernel
<cite>w</cite> is close to the identity kernel, and penalizes otherwise.</p>
<p>f = 1/2 ||T * (v - delta)||^2</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>method</strong> – “fft” for Fast Fourier Transform or “direct” for the
Levinson-Durbin recurssion algorithm. Defaults to “fft”</p></li>
<li><p><strong>optional</strong> – “fft” for Fast Fourier Transform or “direct” for the
Levinson-Durbin recurssion algorithm. Defaults to “fft”</p></li>
<li><p><strong>filter_dim</strong> – the dimensionality of the filter. This parameter should be
upper-bounded by the dimensionality of the data. If data is
3-dimensional and filter_dim is set to 2, one filter is computed
per channel dimension assuming format [B, NC, H , W]. Current
implementation only supports filter dimensions for 1D, 2D and 3D.
Defaults to 2</p></li>
<li><p><strong>optional</strong> – the dimensionality of the filter. This parameter should be
upper-bounded by the dimensionality of the data. If data is
3-dimensional and filter_dim is set to 2, one filter is computed
per channel dimension assuming format [B, NC, H , W]. Current
implementation only supports filter dimensions for 1D, 2D and 3D.
Defaults to 2</p></li>
<li><p><strong>filter_scale</strong> – the scale of the filters compared to the size of the data.
Defaults to 2</p></li>
<li><p><strong>optional</strong> – the scale of the filters compared to the size of the data.
Defaults to 2</p></li>
<li><p><strong>reduction</strong> – specifies the reduction to apply to the output, “mean” or “sum”.
Defaults to mean</p></li>
<li><p><strong>optional</strong> – specifies the reduction to apply to the output, “mean” or “sum”.
Defaults to mean</p></li>
<li><p><strong>mode</strong> – “forward” or “reverse” computation of the filter. For details of
the difference, refer to the original paper. Default “reverse”</p></li>
<li><p><strong>optional</strong> – “forward” or “reverse” computation of the filter. For details of
the difference, refer to the original paper. Default “reverse”</p></li>
<li><p><strong>penalty_function</strong> – the penalty function to apply to the filter. If None, the penalty
function is the identity. Takes “identity”, “gaussian” or custom
penalty function. Default None</p></li>
<li><p><strong>optional</strong> – the penalty function to apply to the filter. If None, the penalty
function is the identity. Takes “identity”, “gaussian” or custom
penalty function. Default None</p></li>
<li><p><strong>std</strong> – the standard deviation of the gaussian when penalty_function=”gaussian”.
Mean is always zero. Default None</p></li>
<li><p><strong>optional</strong> – the standard deviation of the gaussian when penalty_function=”gaussian”.
Mean is always zero. Default None</p></li>
<li><p><strong>store_filters</strong> – whether to store the filters in memory, useful for debugging.
Option to store the filers before or after normalisation with
“norm” and “unorm”. Default False.</p></li>
<li><p><strong>optional</strong> – whether to store the filters in memory, useful for debugging.
Option to store the filers before or after normalisation with
“norm” and “unorm”. Default False.</p></li>
<li><p><strong>epsilon</strong> – the stabilization value to compute the filter. Default 1e-4.</p></li>
<li><p><strong>optional</strong> – the stabilization value to compute the filter. Default 1e-4.</p></li>
<li><p><strong>clamp_min</strong> – filters are clipped to this minimum value after computation. If
None, operation is disabled. Default none</p></li>
<li><p><strong>optional</strong> – filters are clipped to this minimum value after computation. If
None, operation is disabled. Default none</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.utils.wienerloss.WienerLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recon</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/wienerloss.html#WienerLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.wienerloss.WienerLoss.forward" title="Link to this definition">¶</a></dt>
<dd><p>&gt; The function takes in a reconstructed signal, a target signal,
and a few other parameters, and returns the loss</p>
<dl class="simple">
<dt>Args</dt><dd><dl class="simple">
<dt>recon</dt><dd><p>the reconstructed signal</p>
</dd>
<dt>target</dt><dd><p>the target signal</p>
</dd>
<dt>epsilon, optional</dt><dd><p>the stabilization value to compute the filter. If passed,
overwrites the class attribute of same name. Default None.</p>
</dd>
<dt>gamma, optional</dt><dd><p>noise to add to both target and reconstructed signals
for training stabilization. Default 0.</p>
</dd>
<dt>eta, optional</dt><dd><p>noise to add to penalty function. Default 0.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.utils.wienerloss.WienerLoss.get_filter_shape">
<span class="sig-name descname"><span class="pre">get_filter_shape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_shape</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/wienerloss.html#WienerLoss.get_filter_shape"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.wienerloss.WienerLoss.get_filter_shape" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.utils.wienerloss.WienerLoss.identity">
<span class="sig-name descname"><span class="pre">identity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mesh</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/wienerloss.html#WienerLoss.identity"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.wienerloss.WienerLoss.identity" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.utils.wienerloss.WienerLoss.make_delta">
<span class="sig-name descname"><span class="pre">make_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/wienerloss.html#WienerLoss.make_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.wienerloss.WienerLoss.make_delta" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.utils.wienerloss.WienerLoss.make_doubly_block">
<span class="sig-name descname"><span class="pre">make_doubly_block</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/wienerloss.html#WienerLoss.make_doubly_block"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.wienerloss.WienerLoss.make_doubly_block" title="Link to this definition">¶</a></dt>
<dd><p>Makes Doubly Blocked Toeplitz of a matrix X [r, c]</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.utils.wienerloss.WienerLoss.make_penalty">
<span class="sig-name descname"><span class="pre">make_penalty</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">penalty_function</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">std</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/wienerloss.html#WienerLoss.make_penalty"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.wienerloss.WienerLoss.make_penalty" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.utils.wienerloss.WienerLoss.make_toeplitz">
<span class="sig-name descname"><span class="pre">make_toeplitz</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">a</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/wienerloss.html#WienerLoss.make_toeplitz"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.wienerloss.WienerLoss.make_toeplitz" title="Link to this definition">¶</a></dt>
<dd><p>Makes toeplitz matrix of a vector A</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.utils.wienerloss.WienerLoss.multigauss">
<span class="sig-name descname"><span class="pre">multigauss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mesh</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">covmatrix</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/wienerloss.html#WienerLoss.multigauss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.wienerloss.WienerLoss.multigauss" title="Link to this definition">¶</a></dt>
<dd><p>Multivariate gaussian of N dimensions on evenly spaced
hypercubed grid. Mesh should be stacked along the last axis
E.g. for a 3D gaussian of 20 grid points in each axis mesh
should be of shape (20, 20, 20, 3)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.utils.wienerloss.WienerLoss.pad_signal">
<span class="sig-name descname"><span class="pre">pad_signal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/wienerloss.html#WienerLoss.pad_signal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.wienerloss.WienerLoss.pad_signal" title="Link to this definition">¶</a></dt>
<dd><p>x must be a multichannel signal of shape
[batch_size, nchannels, width, height]</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.utils.wienerloss.WienerLoss.rms">
<span class="sig-name descname"><span class="pre">rms</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/wienerloss.html#WienerLoss.rms"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.wienerloss.WienerLoss.rms" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.utils.wienerloss.WienerLoss.wiener">
<span class="sig-name descname"><span class="pre">wiener</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-09</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/wienerloss.html#WienerLoss.wiener"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.wienerloss.WienerLoss.wiener" title="Link to this definition">¶</a></dt>
<dd><p>calculates the optimal least squares convolutional Wiener filter that
transforms signal x into signal y using the direct Toeplitz matrix
implementation</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.utils.wienerloss.WienerLoss.wienerfft">
<span class="sig-name descname"><span class="pre">wienerfft</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prwh</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-09</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/wienerloss.html#WienerLoss.wienerfft"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.wienerloss.WienerLoss.wienerfft" title="Link to this definition">¶</a></dt>
<dd><p>George Strong (<a class="reference external" href="mailto:geowstrong&#37;&#52;&#48;gmail&#46;com">geowstrong<span>&#64;</span>gmail<span>&#46;</span>com</a>)
calculates the optimal least squares convolutional Wiener filter that
transforms signal x into signal y using FFT</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-wiener_transformer.utils.vocab">
<span id="vocabulary"></span><h2>Vocabulary<a class="headerlink" href="#module-wiener_transformer.utils.vocab" title="Link to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="wiener_transformer.utils.vocab.batch_iterator">
<span class="sig-prename descclassname"><span class="pre">wiener_transformer.utils.vocab.</span></span><span class="sig-name descname"><span class="pre">batch_iterator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lang</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'en'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/vocab.html#batch_iterator"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.vocab.batch_iterator" title="Link to this definition">¶</a></dt>
<dd><p>Generator function that yields batches of text data from the dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> – The dataset to iterate over.</p></li>
<li><p><strong>lang</strong> – The language to extract text from (‘en’ or ‘de’).</p></li>
<li><p><strong>batch_size</strong> – The size of each batch to yield.</p></li>
</ul>
</dd>
<dt class="field-even">Yields<span class="colon">:</span></dt>
<dd class="field-even"><p>Batches of text data.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="wiener_transformer.utils.vocab.load_tokenizers">
<span class="sig-prename descclassname"><span class="pre">wiener_transformer.utils.vocab.</span></span><span class="sig-name descname"><span class="pre">load_tokenizers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/vocab.html#load_tokenizers"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.vocab.load_tokenizers" title="Link to this definition">¶</a></dt>
<dd><p>Loads or trains tokenizers for the source and target languages. If pre-trained tokenizers are available on disk,
they are loaded; otherwise, new tokenizers are trained on the dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dataset</strong> – The dataset to use for training the tokenizers if needed.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The source and target tokenizers.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="wiener_transformer.utils.vocab.load_wmt">
<span class="sig-prename descclassname"><span class="pre">wiener_transformer.utils.vocab.</span></span><span class="sig-name descname"><span class="pre">load_wmt</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">language_pair</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">('en',</span> <span class="pre">'de')</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/vocab.html#load_wmt"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.vocab.load_wmt" title="Link to this definition">¶</a></dt>
<dd><p>Loads the WMT dataset for the specified language pair using the HuggingFace datasets library.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>language_pair</strong> – A tuple specifying the source and target languages (default is (‘en’, ‘de’)).</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The training, validation, and test splits of the WMT dataset.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="wiener_transformer.utils.vocab.transform">
<span class="sig-prename descclassname"><span class="pre">wiener_transformer.utils.vocab.</span></span><span class="sig-name descname"><span class="pre">transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">examples</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/vocab.html#transform"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.vocab.transform" title="Link to this definition">¶</a></dt>
<dd><p>Transforms the dataset examples into a format suitable for tokenization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>examples</strong> – A dictionary containing ‘translation’ key with text data.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A dictionary with separate keys for source (‘en’) and target (‘de’) languages.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-wiener_transformer.utils.helpers">
<span id="helpers"></span><h2>Helpers<a class="headerlink" href="#module-wiener_transformer.utils.helpers" title="Link to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="wiener_transformer.utils.helpers.Batch">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">wiener_transformer.utils.helpers.</span></span><span class="sig-name descname"><span class="pre">Batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">src</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/helpers.html#Batch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.helpers.Batch" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Object for holding a batch of data with masks during training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src</strong> – Source sequences for the batch.</p></li>
<li><p><strong>tgt</strong> – Target sequences for the batch (default: None).</p></li>
<li><p><strong>pad</strong> – Index used for padding (default: 2).</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="wiener_transformer.utils.helpers.Batch.src">
<span class="sig-name descname"><span class="pre">src</span></span><a class="headerlink" href="#wiener_transformer.utils.helpers.Batch.src" title="Link to this definition">¶</a></dt>
<dd><p>Source sequences for the batch.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="wiener_transformer.utils.helpers.Batch.src_mask">
<span class="sig-name descname"><span class="pre">src_mask</span></span><a class="headerlink" href="#wiener_transformer.utils.helpers.Batch.src_mask" title="Link to this definition">¶</a></dt>
<dd><p>Mask indicating non-padding elements in the source sequences.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="wiener_transformer.utils.helpers.Batch.tgt">
<span class="sig-name descname"><span class="pre">tgt</span></span><a class="headerlink" href="#wiener_transformer.utils.helpers.Batch.tgt" title="Link to this definition">¶</a></dt>
<dd><p>Target sequences without the last token (teacher forcing).</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="wiener_transformer.utils.helpers.Batch.tgt_y">
<span class="sig-name descname"><span class="pre">tgt_y</span></span><a class="headerlink" href="#wiener_transformer.utils.helpers.Batch.tgt_y" title="Link to this definition">¶</a></dt>
<dd><p>Target sequences without the first token (teacher forcing).</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="wiener_transformer.utils.helpers.Batch.tgt_mask">
<span class="sig-name descname"><span class="pre">tgt_mask</span></span><a class="headerlink" href="#wiener_transformer.utils.helpers.Batch.tgt_mask" title="Link to this definition">¶</a></dt>
<dd><p>Mask indicating non-padding elements and future positions in the target sequences.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="wiener_transformer.utils.helpers.Batch.ntokens">
<span class="sig-name descname"><span class="pre">ntokens</span></span><a class="headerlink" href="#wiener_transformer.utils.helpers.Batch.ntokens" title="Link to this definition">¶</a></dt>
<dd><p>Total number of non-padding tokens in the target sequences.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.utils.helpers.Batch.make_std_mask">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">make_std_mask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tgt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/helpers.html#Batch.make_std_mask"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.helpers.Batch.make_std_mask" title="Link to this definition">¶</a></dt>
<dd><p>Create a mask to hide padding and future words in the target sequences.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tgt</strong> – Target sequences.</p></li>
<li><p><strong>pad</strong> – Index used for padding.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tensor representing the mask for the target sequences.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="wiener_transformer.utils.helpers.DummyOptimizer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">wiener_transformer.utils.helpers.</span></span><span class="sig-name descname"><span class="pre">DummyOptimizer</span></span><a class="reference internal" href="_modules/wiener_transformer/utils/helpers.html#DummyOptimizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.helpers.DummyOptimizer" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code></p>
<p>A dummy optimizer that does nothing. Used for testing or evaluation without updates.</p>
<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.utils.helpers.DummyOptimizer.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/helpers.html#DummyOptimizer.step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.helpers.DummyOptimizer.step" title="Link to this definition">¶</a></dt>
<dd><p>Placeholder method to simulate an optimizer step.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.utils.helpers.DummyOptimizer.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">set_to_none</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/helpers.html#DummyOptimizer.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.helpers.DummyOptimizer.zero_grad" title="Link to this definition">¶</a></dt>
<dd><p>Placeholder method to simulate zeroing gradients.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id0">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/helpers.html#DummyOptimizer.step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#id0" title="Link to this definition">¶</a></dt>
<dd><p>Performs a single optimization step (parameter update).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>closure</strong> (<em>Callable</em>) – A closure that reevaluates the model and
returns the loss. Optional for most optimizers.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Unless otherwise specified, this function should not modify the
<code class="docutils literal notranslate"><span class="pre">.grad</span></code> field of the parameters.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id1">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">set_to_none</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/helpers.html#DummyOptimizer.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#id1" title="Link to this definition">¶</a></dt>
<dd><p>Resets the gradients of all optimized <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code> s.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>set_to_none</strong> (<em>bool</em>) – instead of setting to zero, set the grads to None.
This will in general have lower memory footprint, and can modestly improve performance.
However, it changes certain behaviors. For example:
1. When the user tries to access a gradient and perform manual ops on it,
a None attribute or a Tensor full of 0s will behave differently.
2. If the user requests <code class="docutils literal notranslate"><span class="pre">zero_grad(set_to_none=True)</span></code> followed by a backward pass, <code class="docutils literal notranslate"><span class="pre">.grad</span></code>s
are guaranteed to be None for params that did not receive a gradient.
3. <code class="docutils literal notranslate"><span class="pre">torch.optim</span></code> optimizers have a different behavior if the gradient is 0 or None
(in one case it does the step with a gradient of 0 and in the other it skips
the step altogether).</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="wiener_transformer.utils.helpers.DummyScheduler">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">wiener_transformer.utils.helpers.</span></span><span class="sig-name descname"><span class="pre">DummyScheduler</span></span><a class="reference internal" href="_modules/wiener_transformer/utils/helpers.html#DummyScheduler"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.helpers.DummyScheduler" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A dummy learning rate scheduler that does nothing. Used for testing or evaluation without updates.</p>
<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.utils.helpers.DummyScheduler.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/helpers.html#DummyScheduler.step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.helpers.DummyScheduler.step" title="Link to this definition">¶</a></dt>
<dd><p>Placeholder method to simulate a scheduler step.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="id2">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/helpers.html#DummyScheduler.step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#id2" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="wiener_transformer.utils.helpers.TrainState">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">wiener_transformer.utils.helpers.</span></span><span class="sig-name descname"><span class="pre">TrainState</span></span><a class="reference internal" href="_modules/wiener_transformer/utils/helpers.html#TrainState"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.helpers.TrainState" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Track the number of steps, examples, and tokens processed during training.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="wiener_transformer.utils.helpers.TrainState.step">
<span class="sig-name descname"><span class="pre">step</span></span><a class="headerlink" href="#wiener_transformer.utils.helpers.TrainState.step" title="Link to this definition">¶</a></dt>
<dd><p>Number of steps in the current epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="wiener_transformer.utils.helpers.TrainState.accum_step">
<span class="sig-name descname"><span class="pre">accum_step</span></span><a class="headerlink" href="#wiener_transformer.utils.helpers.TrainState.accum_step" title="Link to this definition">¶</a></dt>
<dd><p>Number of gradient accumulation steps.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="wiener_transformer.utils.helpers.TrainState.samples">
<span class="sig-name descname"><span class="pre">samples</span></span><a class="headerlink" href="#wiener_transformer.utils.helpers.TrainState.samples" title="Link to this definition">¶</a></dt>
<dd><p>Total number of examples used.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="wiener_transformer.utils.helpers.TrainState.tokens">
<span class="sig-name descname"><span class="pre">tokens</span></span><a class="headerlink" href="#wiener_transformer.utils.helpers.TrainState.tokens" title="Link to this definition">¶</a></dt>
<dd><p>Total number of tokens processed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id3">
<span class="sig-name descname"><span class="pre">accum_step</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0</span></em><a class="headerlink" href="#id3" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id4">
<span class="sig-name descname"><span class="pre">samples</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0</span></em><a class="headerlink" href="#id4" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id5">
<span class="sig-name descname"><span class="pre">step</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0</span></em><a class="headerlink" href="#id5" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id6">
<span class="sig-name descname"><span class="pre">tokens</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">0</span></em><a class="headerlink" href="#id6" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="wiener_transformer.utils.helpers.calculate_bleu">
<span class="sig-prename descclassname"><span class="pre">wiener_transformer.utils.helpers.</span></span><span class="sig-name descname"><span class="pre">calculate_bleu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_dataloader</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_tokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/helpers.html#calculate_bleu"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.helpers.calculate_bleu" title="Link to this definition">¶</a></dt>
<dd><p>Calculate BLEU scores and BERTScore for model predictions on the test dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> – The trained model to be evaluated.</p></li>
<li><p><strong>test_dataloader</strong> – DataLoader providing batches of test data.</p></li>
<li><p><strong>pad_idx</strong> – The index used for padding in the sequences.</p></li>
<li><p><strong>tgt_tokenizer</strong> – Tokenizer for the target language.</p></li>
<li><p><strong>max_len</strong> – Maximum length for generated sequences (default: 50).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p>corpus_bleu_score: BLEU score for the entire corpus.</p></li>
<li><p>average_sentence_bleu_score: Average BLEU score for individual sentences.</p></li>
<li><p>sacrebleu_score: SacreBLEU score for the corpus.</p></li>
<li><p>recall: Average BERTScore recall.</p></li>
<li><p>precision: Average BERTScore precision.</p></li>
<li><p>f1: Average BERTScore F1 score.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tuple containing</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="wiener_transformer.utils.helpers.pad">
<span class="sig-prename descclassname"><span class="pre">wiener_transformer.utils.helpers.</span></span><span class="sig-name descname"><span class="pre">pad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/helpers.html#pad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.helpers.pad" title="Link to this definition">¶</a></dt>
<dd><p>Pad a tensor with a specified value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> – The tensor to be padded.</p></li>
<li><p><strong>pad</strong> – A tuple specifying the padding to be applied to each dimension.</p></li>
<li><p><strong>value</strong> – The value to use for padding.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Padded tensor.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="wiener_transformer.utils.helpers.rate">
<span class="sig-prename descclassname"><span class="pre">wiener_transformer.utils.helpers.</span></span><span class="sig-name descname"><span class="pre">rate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">step</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">factor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warmup</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/helpers.html#rate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.helpers.rate" title="Link to this definition">¶</a></dt>
<dd><p>Compute the learning rate according to the warmup strategy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>step</strong> – Current step number.</p></li>
<li><p><strong>model_size</strong> – Dimensionality of the model.</p></li>
<li><p><strong>factor</strong> – Scaling factor for the learning rate.</p></li>
<li><p><strong>warmup</strong> – Number of warmup steps.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Learning rate value for the current step.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="wiener_transformer.utils.helpers.run_epoch">
<span class="sig-prename descclassname"><span class="pre">wiener_transformer.utils.helpers.</span></span><span class="sig-name descname"><span class="pre">run_epoch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_iter</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wiener_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classic_loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scheduler</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">accelerator</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode='train'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_weight=1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">accum_iter=1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_state=&lt;wiener_transformer.utils.helpers.TrainState</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_batches=inf</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma=0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">awl=None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/helpers.html#run_epoch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.helpers.run_epoch" title="Link to this definition">¶</a></dt>
<dd><p>Train or evaluate the model for a single epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_iter</strong> – DataLoader providing batches of data.</p></li>
<li><p><strong>model</strong> – The model being trained or evaluated.</p></li>
<li><p><strong>wiener_fn</strong> – Function to compute the Wiener loss (if applicable).</p></li>
<li><p><strong>classic_loss</strong> – Function to compute the standard loss (e.g., cross-entropy).</p></li>
<li><p><strong>optimizer</strong> – Optimizer used for training.</p></li>
<li><p><strong>scheduler</strong> – Learning rate scheduler.</p></li>
<li><p><strong>accelerator</strong> – Distributed training utility (e.g., from Hugging Face’s Accelerate).</p></li>
<li><p><strong>mode</strong> – Mode of operation, either “train” or “eval” (default: “train”).</p></li>
<li><p><strong>loss_weight</strong> – Weighting factor for the loss function (default: 1).</p></li>
<li><p><strong>accum_iter</strong> – Number of steps for gradient accumulation (default: 1).</p></li>
<li><p><strong>train_state</strong> – Object tracking training state (default: TrainState()).</p></li>
<li><p><strong>max_batches</strong> – Maximum number of batches to process (default: inf).</p></li>
<li><p><strong>gamma</strong> – Regularization parameter for the Wiener loss (default: 0.1).</p></li>
<li><p><strong>awl</strong> – Automatic weighting of losses (if applicable).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p>avg_loss: Average combined loss over the epoch.</p></li>
<li><p>avg_wiener: Average Wiener loss over the epoch.</p></li>
<li><p>avg_kldiv: Average Kullback-Leibler divergence loss over the epoch.</p></li>
<li><p>train_state: Updated training state after the epoch.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tuple containing</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="wiener_transformer.utils.helpers.subsequent_mask">
<span class="sig-prename descclassname"><span class="pre">wiener_transformer.utils.helpers.</span></span><span class="sig-name descname"><span class="pre">subsequent_mask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">size</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/helpers.html#subsequent_mask"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.helpers.subsequent_mask" title="Link to this definition">¶</a></dt>
<dd><p>Create a mask to hide subsequent positions in a sequence.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>size</strong> – The length of the sequence.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tensor of shape (1, size, size) with True values in the upper triangular part.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="wiener_transformer.utils.helpers.tokenize">
<span class="sig-prename descclassname"><span class="pre">wiener_transformer.utils.helpers.</span></span><span class="sig-name descname"><span class="pre">tokenize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/helpers.html#tokenize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.helpers.tokenize" title="Link to this definition">¶</a></dt>
<dd><p>Tokenize a given text using a specified tokenizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>text</strong> – The text to be tokenized.</p></li>
<li><p><strong>tokenizer</strong> – The tokenizer to be used for tokenization.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>List of tokens.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-wiener_transformer.utils.data_loader">
<span id="data-loader"></span><h2>Data Loader<a class="headerlink" href="#module-wiener_transformer.utils.data_loader" title="Link to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="wiener_transformer.utils.data_loader.WMT14Dataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">wiener_transformer.utils.data_loader.</span></span><span class="sig-name descname"><span class="pre">WMT14Dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_iter</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/data_loader.html#WMT14Dataset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.data_loader.WMT14Dataset" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></p>
<p>A custom Dataset class for the WMT14 dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>data_iter</strong> – An iterable containing the data samples.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.utils.data_loader.WMT14Dataset.__len__">
<span class="sig-name descname"><span class="pre">__len__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/data_loader.html#WMT14Dataset.__len__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.data_loader.WMT14Dataset.__len__" title="Link to this definition">¶</a></dt>
<dd><p>Returns the length of the dataset.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="wiener_transformer.utils.data_loader.WMT14Dataset.__getitem__">
<span class="sig-name descname"><span class="pre">__getitem__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/data_loader.html#WMT14Dataset.__getitem__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.data_loader.WMT14Dataset.__getitem__" title="Link to this definition">¶</a></dt>
<dd><p>Returns the data sample at the given index.</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="wiener_transformer.utils.data_loader.collate_batch">
<span class="sig-prename descclassname"><span class="pre">wiener_transformer.utils.data_loader.</span></span><span class="sig-name descname"><span class="pre">collate_batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_pipeline</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_pipeline</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_id</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/data_loader.html#collate_batch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.data_loader.collate_batch" title="Link to this definition">¶</a></dt>
<dd><p>Collate a batch of data for the DataLoader, applying tokenization, padding, and conversion to tensors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> – A list of data examples where each example is a dictionary containing ‘translation’ with ‘en’ and ‘de’ keys.</p></li>
<li><p><strong>src_pipeline</strong> – A tokenization function for the source text.</p></li>
<li><p><strong>tgt_pipeline</strong> – A tokenization function for the target text.</p></li>
<li><p><strong>device</strong> – The device to place the tensors on (e.g., ‘cpu’ or ‘cuda’).</p></li>
<li><p><strong>max_padding</strong> – The maximum length to pad the sequences to (default: 128).</p></li>
<li><p><strong>pad_id</strong> – The token ID used for padding (default: 2).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p>src: Padded and tokenized source sequences.</p></li>
<li><p>tgt: Padded and tokenized target sequences.</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>A tuple of tensors (src, tgt) where</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="wiener_transformer.utils.data_loader.create_dataloaders">
<span class="sig-prename descclassname"><span class="pre">wiener_transformer.utils.data_loader.</span></span><span class="sig-name descname"><span class="pre">create_dataloaders</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">valid_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_tokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_tokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/wiener_transformer/utils/data_loader.html#create_dataloaders"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#wiener_transformer.utils.data_loader.create_dataloaders" title="Link to this definition">¶</a></dt>
<dd><p>Create DataLoader objects for the training, validation, and test datasets.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_data</strong> – Iterable containing the training data.</p></li>
<li><p><strong>valid_data</strong> – Iterable containing the validation data.</p></li>
<li><p><strong>test_data</strong> – Iterable containing the test data.</p></li>
<li><p><strong>src_tokenizer</strong> – Tokenizer for the source language.</p></li>
<li><p><strong>tgt_tokenizer</strong> – Tokenizer for the target language.</p></li>
<li><p><strong>device</strong> – The device to place the tensors on (e.g., ‘cpu’ or ‘cuda’).</p></li>
<li><p><strong>batch_size</strong> – Number of samples per batch (default: 32).</p></li>
<li><p><strong>max_padding</strong> – The maximum length to pad the sequences to (default: 128).</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tuple containing DataLoader objects for the training, validation, and test datasets.</p>
</dd>
</dl>
</dd></dl>

</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">Wiener Transformer</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Wiener Transformer</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#wiener_transformer.transformer.Decoder"><code class="docutils literal notranslate"><span class="pre">Decoder</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#wiener_transformer.transformer.DecoderLayer"><code class="docutils literal notranslate"><span class="pre">DecoderLayer</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#wiener_transformer.transformer.Embeddings"><code class="docutils literal notranslate"><span class="pre">Embeddings</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#wiener_transformer.transformer.Encoder"><code class="docutils literal notranslate"><span class="pre">Encoder</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#wiener_transformer.transformer.EncoderDecoder"><code class="docutils literal notranslate"><span class="pre">EncoderDecoder</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#wiener_transformer.transformer.EncoderLayer"><code class="docutils literal notranslate"><span class="pre">EncoderLayer</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#wiener_transformer.transformer.Generator"><code class="docutils literal notranslate"><span class="pre">Generator</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#wiener_transformer.transformer.LayerNorm"><code class="docutils literal notranslate"><span class="pre">LayerNorm</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#wiener_transformer.transformer.MultiHeadedAttention"><code class="docutils literal notranslate"><span class="pre">MultiHeadedAttention</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#wiener_transformer.transformer.PositionalEncoding"><code class="docutils literal notranslate"><span class="pre">PositionalEncoding</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#wiener_transformer.transformer.PositionwiseFeedForward"><code class="docutils literal notranslate"><span class="pre">PositionwiseFeedForward</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#wiener_transformer.transformer.SublayerConnection"><code class="docutils literal notranslate"><span class="pre">SublayerConnection</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#wiener_transformer.transformer.clones"><code class="docutils literal notranslate"><span class="pre">clones()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#wiener_transformer.transformer.make_model"><code class="docutils literal notranslate"><span class="pre">make_model()</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#wiener_transformer.transformer.subsequent_mask"><code class="docutils literal notranslate"><span class="pre">subsequent_mask()</span></code></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#utilities">Utilities</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#module-wiener_transformer.utils.train">Train</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-wiener_transformer.utils.embeddings">Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-wiener_transformer.utils.wienerloss">Wiener Loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-wiener_transformer.utils.vocab">Vocabulary</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-wiener_transformer.utils.helpers">Helpers</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-wiener_transformer.utils.data_loader">Data Loader</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="wiener_attention.html">Wiener Attention</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="index.html" title="previous chapter">Welcome to Wiener Transformer Documentation</a></li>
      <li>Next: <a href="wiener_attention.html" title="next chapter">Wiener Attention</a></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2024, Andrei Danila.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.0.2</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="_sources/wiener_transformer.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>